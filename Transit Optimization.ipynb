{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class MLTransitAnalyzer:\n",
    "    \"\"\"A tool for analyzing public transit GTFS data with machine learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        \"\"\"Initialize the analyzer with a data directory.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.models_dir = os.path.join(data_dir, \"models\")\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "        \n",
    "        # Storage for loaded data\n",
    "        self.routes = None\n",
    "        self.trips = None\n",
    "        self.stop_times = None\n",
    "        self.stops = None\n",
    "        self.calendar = None\n",
    "        \n",
    "        # Storage for derived data\n",
    "        self.route_stats = None\n",
    "        self.stop_stats = None\n",
    "        self.time_stats = None\n",
    "        \n",
    "        # Machine learning models\n",
    "        self.demand_model = None\n",
    "        self.demand_features = None\n",
    "        self.scaler = None\n",
    "        self.stop_clusters = None\n",
    "        \n",
    "    def download_gtfs(self, url):\n",
    "        \"\"\"Download GTFS data from a URL.\"\"\"\n",
    "        print(f\"Downloading GTFS data from {url}...\")\n",
    "        \n",
    "        # Download the zip file\n",
    "        response = requests.get(url)\n",
    "        zip_path = os.path.join(self.data_dir, \"gtfs.zip\")\n",
    "        \n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Extract the files\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.data_dir)\n",
    "        \n",
    "        print(\"GTFS data downloaded and extracted successfully.\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load GTFS data from the data directory.\"\"\"\n",
    "        print(\"Loading GTFS data...\")\n",
    "        \n",
    "        # Load the GTFS files\n",
    "        try:\n",
    "            self.routes = pd.read_csv(os.path.join(self.data_dir, \"routes.txt\"))\n",
    "            self.trips = pd.read_csv(os.path.join(self.data_dir, \"trips.txt\"))\n",
    "            self.stop_times = pd.read_csv(os.path.join(self.data_dir, \"stop_times.txt\"))\n",
    "            self.stops = pd.read_csv(os.path.join(self.data_dir, \"stops.txt\"))\n",
    "            self.calendar = pd.read_csv(os.path.join(self.data_dir, \"calendar.txt\"))\n",
    "            \n",
    "            print(f\"Loaded {len(self.routes)} routes and {len(self.stops)} stops.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the data for analysis and machine learning.\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        if any(x is None for x in [self.routes, self.trips, self.stop_times, self.stops, self.calendar]):\n",
    "            print(\"Data not loaded. Please load data first.\")\n",
    "            return False\n",
    "        if self.routes is not None:\n",
    "            self.routes['route_id'] = self.routes['route_id'].astype(str)\n",
    "        if self.trips is not None:\n",
    "            self.trips['route_id'] = self.trips['route_id'].astype(str)\n",
    "            self.trips['trip_id'] = self.trips['trip_id'].astype(str)\n",
    "        if self.stop_times is not None:\n",
    "            self.stop_times['trip_id'] = self.stop_times['trip_id'].astype(str)\n",
    "            self.stop_times['stop_id'] = self.stop_times['stop_id'].astype(str)\n",
    "        if self.stops is not None:\n",
    "            self.stops['stop_id'] = self.stops['stop_id'].astype(str)        \n",
    "        \n",
    "        # 1. Extract hour from departure_time\n",
    "        def extract_hour(time_str):\n",
    "            try:\n",
    "                parts = time_str.split(':')\n",
    "                hour = int(parts[0])\n",
    "                if hour >= 24: \n",
    "                    hour = hour - 24\n",
    "                return hour\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        self.stop_times['departure_hour'] = self.stop_times['departure_time'].apply(extract_hour)\n",
    "        \n",
    "        # 2. Compute route statistics\n",
    "        self.route_stats = self.analyze_route_frequency()\n",
    "        \n",
    "        # 3. Compute stop statistics\n",
    "        self.stop_stats = self.analyze_stop_activity()\n",
    "        \n",
    "        # 4. Compute time statistics\n",
    "        hourly_counts = self.stop_times.groupby('departure_hour').size()\n",
    "        self.time_stats = pd.DataFrame({\n",
    "            'hour': hourly_counts.index,\n",
    "            'trip_count': hourly_counts.values\n",
    "        })\n",
    "        \n",
    "        # 5. Join trips with calendar for day of week analysis\n",
    "        trips_calendar = pd.merge(self.trips, self.calendar, on='service_id')\n",
    "        \n",
    "        # 6. Create a more detailed stop times dataset with trip and route info\n",
    "        detailed_stop_times = pd.merge(\n",
    "            self.stop_times,\n",
    "            self.trips[['trip_id', 'route_id', 'service_id']],\n",
    "            on='trip_id'\n",
    "        )\n",
    "        \n",
    "        # 7. Add day of week indicators\n",
    "        for day in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']:\n",
    "            detailed_stop_times[day] = detailed_stop_times['service_id'].map(\n",
    "                self.calendar.set_index('service_id')[day]\n",
    "            )\n",
    "        \n",
    "        # 8. Create hourly demand by stop dataset for machine learning\n",
    "        stop_hour_demand = detailed_stop_times.groupby(['stop_id', 'departure_hour']).size().reset_index(name='demand')\n",
    "        \n",
    "        # 9. Add stop location information\n",
    "        stop_hour_demand = pd.merge(stop_hour_demand, self.stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
    "        \n",
    "        # 10. Add additional features for machine learning\n",
    "        stop_hour_demand['is_rush_hour'] = ((stop_hour_demand['departure_hour'] >= 7) & \n",
    "                                            (stop_hour_demand['departure_hour'] <= 9) | \n",
    "                                            (stop_hour_demand['departure_hour'] >= 16) & \n",
    "                                            (stop_hour_demand['departure_hour'] <= 18)).astype(int)\n",
    "        \n",
    "        stop_hour_demand['is_business_hours'] = ((stop_hour_demand['departure_hour'] >= 9) & \n",
    "                                                (stop_hour_demand['departure_hour'] <= 17)).astype(int)\n",
    "        \n",
    "        stop_hour_demand['is_night'] = ((stop_hour_demand['departure_hour'] >= 22) | \n",
    "                                        (stop_hour_demand['departure_hour'] <= 5)).astype(int)\n",
    "        \n",
    "        # Store this dataset for machine learning\n",
    "        self.demand_features = stop_hour_demand\n",
    "        \n",
    "        print(\"Data preprocessing complete.\")\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def analyze_route_frequency(self):\n",
    "        \"\"\"Analyze the frequency of service by route.\"\"\"\n",
    "        if self.trips is None or self.routes is None:\n",
    "            print(\"Data not loaded. Please load data first.\")\n",
    "            return None\n",
    "\n",
    "        # Count trips per route\n",
    "        trip_counts = self.trips.groupby('route_id').size().reset_index(name='trip_count')\n",
    "\n",
    "        # Convert route_id to string in both dataframes to ensure type consistency\n",
    "        trip_counts['route_id'] = trip_counts['route_id'].astype(str)\n",
    "        self.routes['route_id'] = self.routes['route_id'].astype(str)\n",
    "\n",
    "        # Merge with route information\n",
    "        try:\n",
    "            # Try to merge with route_long_name which is the standard field\n",
    "            route_frequency = pd.merge(trip_counts, self.routes[['route_id', 'route_long_name']], on='route_id')\n",
    "        except KeyError:\n",
    "            # Some GTFS feeds use route_short_name instead of route_long_name\n",
    "            print(\"Using route_short_name instead of route_long_name\")\n",
    "            if 'route_short_name' in self.routes.columns:\n",
    "                route_frequency = pd.merge(trip_counts, self.routes[['route_id', 'route_short_name']], on='route_id')\n",
    "                route_frequency = route_frequency.rename(columns={'route_short_name': 'route_long_name'})\n",
    "            else:\n",
    "                # If neither is available, just use route_id\n",
    "                route_frequency = trip_counts.copy()\n",
    "                route_frequency['route_long_name'] = route_frequency['route_id']\n",
    "\n",
    "        # Sort by trip count\n",
    "        route_frequency = route_frequency.sort_values('trip_count', ascending=False)\n",
    "\n",
    "        return route_frequency\n",
    "    \n",
    "    def analyze_stop_activity(self):\n",
    "        \"\"\"Analyze the activity level at each stop.\"\"\"\n",
    "        if self.stop_times is None or self.stops is None:\n",
    "            print(\"Data not loaded. Please load data first.\")\n",
    "            return None\n",
    "        \n",
    "        # Count stop occurrences in stop_times\n",
    "        stop_counts = self.stop_times['stop_id'].value_counts().reset_index()\n",
    "        stop_counts.columns = ['stop_id', 'activity_count']\n",
    "        \n",
    "        # Merge with stop information\n",
    "        stop_activity = pd.merge(stop_counts, self.stops[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']], on='stop_id')\n",
    "        \n",
    "        # Sort by activity count\n",
    "        stop_activity = stop_activity.sort_values('activity_count', ascending=False)\n",
    "        \n",
    "        return stop_activity\n",
    "    \n",
    "    def cluster_stops_by_location(self, n_clusters=5):\n",
    "        \"\"\"\n",
    "        Cluster stops by geographic location to identify natural service areas.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_clusters : int\n",
    "            Number of clusters to create\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        stop_clusters : DataFrame\n",
    "            Stops with their assigned clusters\n",
    "        \"\"\"\n",
    "        print(f\"Clustering stops into {n_clusters} service areas...\")\n",
    "        \n",
    "        if self.stop_stats is None:\n",
    "            self.stop_stats = self.analyze_stop_activity()\n",
    "            \n",
    "        if self.stop_stats is None:\n",
    "            print(\"Unable to get stop activity data.\")\n",
    "            return None\n",
    "        \n",
    "        # Extract location data\n",
    "        stop_locations = self.stop_stats[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'activity_count']]\n",
    "        \n",
    "        # Use K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        stop_locations['cluster'] = kmeans.fit_predict(stop_locations[['stop_lat', 'stop_lon']])\n",
    "        \n",
    "        # Calculate cluster statistics\n",
    "        cluster_stats = stop_locations.groupby('cluster').agg({\n",
    "            'stop_id': 'count',\n",
    "            'activity_count': 'sum',\n",
    "            'stop_lat': 'mean',\n",
    "            'stop_lon': 'mean'\n",
    "        }).rename(columns={'stop_id': 'stop_count'})\n",
    "        \n",
    "        print(\"Service area clustering complete.\")\n",
    "        \n",
    "        # Store the clusters\n",
    "        self.stop_clusters = stop_locations\n",
    "        \n",
    "        return stop_locations, cluster_stats\n",
    "    \n",
    "    def train_demand_prediction_model(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train a machine learning model to predict demand at stops based on time and location.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float\n",
    "            Portion of data to use for testing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        model_performance : dict\n",
    "            Model performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Training demand prediction model...\")\n",
    "        \n",
    "        if self.demand_features is None:\n",
    "            print(\"Feature data not available. Run preprocess_data() first.\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = self.demand_features[['stop_lat', 'stop_lon', 'departure_hour', \n",
    "                                'is_rush_hour', 'is_business_hours', 'is_night']]\n",
    "        y = self.demand_features['demand']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        numeric_features = ['stop_lat', 'stop_lon', 'departure_hour']\n",
    "        binary_features = ['is_rush_hour', 'is_business_hours', 'is_night']\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('bin', 'passthrough', binary_features)\n",
    "            ])\n",
    "        \n",
    "        # Create model pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', GradientBoostingRegressor(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Model Performance:\")\n",
    "        print(f\"  MAE: {mae:.2f}\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        \n",
    "        # Store the model\n",
    "        self.demand_model = pipeline\n",
    "        self.scaler = preprocessor\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(pipeline, os.path.join(self.models_dir, 'demand_prediction_model.pkl'))\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'model': pipeline\n",
    "        }\n",
    "    \n",
    "    def predict_stop_demand(self, stop_id, hour):\n",
    "        \"\"\"\n",
    "        Predict demand for a specific stop and hour.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        stop_id : str\n",
    "            ID of the stop\n",
    "        hour : int\n",
    "            Hour of the day (0-23)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predicted_demand : float\n",
    "            Predicted number of trips\n",
    "        \"\"\"\n",
    "        if self.demand_model is None:\n",
    "            print(\"Demand prediction model not trained. Run train_demand_prediction_model() first.\")\n",
    "            return None\n",
    "        \n",
    "        if stop_id not in self.stops['stop_id'].values:\n",
    "            print(f\"Stop ID {stop_id} not found in data.\")\n",
    "            return None\n",
    "        \n",
    "        # Get stop location\n",
    "        stop_info = self.stops[self.stops['stop_id'] == stop_id].iloc[0]\n",
    "        \n",
    "        # Create feature vector\n",
    "        is_rush_hour = 1 if (hour >= 7 and hour <= 9) or (hour >= 16 and hour <= 18) else 0\n",
    "        is_business_hours = 1 if hour >= 9 and hour <= 17 else 0\n",
    "        is_night = 1 if hour >= 22 or hour <= 5 else 0\n",
    "        \n",
    "        features = pd.DataFrame({\n",
    "            'stop_lat': [stop_info['stop_lat']],\n",
    "            'stop_lon': [stop_info['stop_lon']],\n",
    "            'departure_hour': [hour],\n",
    "            'is_rush_hour': [is_rush_hour],\n",
    "            'is_business_hours': [is_business_hours],\n",
    "            'is_night': [is_night]\n",
    "        })\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_demand = self.demand_model.predict(features)[0]\n",
    "        \n",
    "        return max(0, round(predicted_demand, 1))  # Ensure non-negative and round\n",
    "    \n",
    "    def predict_system_demand(self, hour):\n",
    "        \"\"\"\n",
    "        Predict demand across the whole system for a given hour.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hour : int\n",
    "            Hour of the day (0-23)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        demand_predictions : DataFrame\n",
    "            Predicted demand for all stops\n",
    "        \"\"\"\n",
    "        if self.demand_model is None:\n",
    "            print(\"Demand prediction model not trained. Run train_demand_prediction_model() first.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Predicting system-wide demand for hour {hour}...\")\n",
    "        \n",
    "        # Prepare features for all stops\n",
    "        is_rush_hour = 1 if (hour >= 7 and hour <= 9) or (hour >= 16 and hour <= 18) else 0\n",
    "        is_business_hours = 1 if hour >= 9 and hour <= 17 else 0\n",
    "        is_night = 1 if hour >= 22 or hour <= 5 else 0\n",
    "        \n",
    "        # Create a dataframe with a row for each stop\n",
    "        features = pd.DataFrame({\n",
    "            'stop_id': self.stops['stop_id'],\n",
    "            'stop_name': self.stops['stop_name'],\n",
    "            'stop_lat': self.stops['stop_lat'],\n",
    "            'stop_lon': self.stops['stop_lon'],\n",
    "            'departure_hour': hour,\n",
    "            'is_rush_hour': is_rush_hour,\n",
    "            'is_business_hours': is_business_hours,\n",
    "            'is_night': is_night\n",
    "        })\n",
    "        \n",
    "        # Make predictions\n",
    "        X_pred = features[['stop_lat', 'stop_lon', 'departure_hour', \n",
    "                          'is_rush_hour', 'is_business_hours', 'is_night']]\n",
    "        features['predicted_demand'] = self.demand_model.predict(X_pred)\n",
    "        \n",
    "        # Ensure non-negative values\n",
    "        features['predicted_demand'] = features['predicted_demand'].apply(lambda x: max(0, round(x, 1)))\n",
    "        \n",
    "        return features[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'predicted_demand']]\n",
    "    \n",
    "    def identify_underserved_stops(self):\n",
    "        \"\"\"\n",
    "        Identify stops that are likely underserved based on predicted vs. actual demand.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        underserved_stops : DataFrame\n",
    "            Stops with demand vs. service metrics\n",
    "        \"\"\"\n",
    "        print(\"Identifying potentially underserved stops...\")\n",
    "        \n",
    "        if self.demand_model is None or self.stop_stats is None:\n",
    "            print(\"Required data or models not available.\")\n",
    "            return None\n",
    "        \n",
    "        # For each stop, predict demand during peak hours\n",
    "        morning_peak = 8  # 8 AM\n",
    "        evening_peak = 17  # 5 PM\n",
    "        \n",
    "        # Get morning and evening predictions\n",
    "        morning_predictions = self.predict_system_demand(morning_peak)\n",
    "        evening_predictions = self.predict_system_demand(evening_peak)\n",
    "        \n",
    "        # Calculate average peak demand\n",
    "        peak_demand = pd.DataFrame({\n",
    "            'stop_id': morning_predictions['stop_id'],\n",
    "            'stop_name': morning_predictions['stop_name'],\n",
    "            'stop_lat': morning_predictions['stop_lat'],\n",
    "            'stop_lon': morning_predictions['stop_lon'],\n",
    "            'morning_demand': morning_predictions['predicted_demand'],\n",
    "            'evening_demand': evening_predictions['predicted_demand'],\n",
    "            'peak_demand': (morning_predictions['predicted_demand'] + evening_predictions['predicted_demand']) / 2\n",
    "        })\n",
    "        \n",
    "        # Merge with actual service levels\n",
    "        service_comparison = pd.merge(\n",
    "            peak_demand,\n",
    "            self.stop_stats[['stop_id', 'activity_count']],\n",
    "            on='stop_id'\n",
    "        )\n",
    "        \n",
    "        # Calculate demand to service ratio\n",
    "        service_comparison['demand_service_ratio'] = service_comparison['peak_demand'] / service_comparison['activity_count']\n",
    "        \n",
    "        # Identify potentially underserved stops (high demand, low service)\n",
    "        service_comparison['is_underserved'] = (\n",
    "            (service_comparison['peak_demand'] > service_comparison['peak_demand'].median()) & \n",
    "            (service_comparison['activity_count'] < service_comparison['activity_count'].median())\n",
    "        )\n",
    "        \n",
    "        # Sort by demand to service ratio (higher values = more underserved)\n",
    "        underserved_stops = service_comparison.sort_values('demand_service_ratio', ascending=False)\n",
    "        \n",
    "        return underserved_stops\n",
    "    \n",
    "    def create_demand_heatmap(self, hour, filename=\"demand_heatmap.html\"):\n",
    "        \"\"\"\n",
    "        Create an interactive heatmap of predicted demand across the system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hour : int\n",
    "            Hour of the day to predict for (0-23)\n",
    "        filename : str\n",
    "            Name of the output HTML file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        output_path : str\n",
    "            Path to the generated heatmap\n",
    "        \"\"\"\n",
    "        print(f\"Creating demand heatmap for hour {hour}...\")\n",
    "        \n",
    "        # Get demand predictions\n",
    "        predictions = self.predict_system_demand(hour)\n",
    "        \n",
    "        if predictions is None:\n",
    "            return None\n",
    "        \n",
    "        # Create a map centered at the mean location of stops\n",
    "        center_lat = predictions['stop_lat'].mean()\n",
    "        center_lon = predictions['stop_lon'].mean()\n",
    "        \n",
    "        m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "        \n",
    "        # Add a heatmap layer\n",
    "        from folium.plugins import HeatMap\n",
    "        \n",
    "        # Prepare data for heatmap (lat, lon, intensity)\n",
    "        heat_data = [\n",
    "            [row['stop_lat'], row['stop_lon'], row['predicted_demand']] \n",
    "            for _, row in predictions.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Add heatmap to the map\n",
    "        HeatMap(heat_data).add_to(m)\n",
    "        \n",
    "        # Add markers for top demand locations\n",
    "        top_demand = predictions.nlargest(10, 'predicted_demand')\n",
    "        \n",
    "        for _, row in top_demand.iterrows():\n",
    "            folium.Marker(\n",
    "                location=[row['stop_lat'], row['stop_lon']],\n",
    "                popup=f\"Stop: {row['stop_name']}<br>Predicted Demand: {row['predicted_demand']}\",\n",
    "                icon=folium.Icon(color='red', icon='info-sign')\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add time information\n",
    "        time_str = f\"{hour}:00\"\n",
    "        title_html = f'''\n",
    "            <h3 align=\"center\" style=\"font-size:16px\"><b>Predicted Demand at {time_str}</b></h3>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        # Save the map\n",
    "        output_path = os.path.join(self.data_dir, filename)\n",
    "        m.save(output_path)\n",
    "        \n",
    "        print(f\"Demand heatmap saved to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def optimize_service_frequency(self, route_id):\n",
    "        \"\"\"\n",
    "        Recommend optimal service frequency for a route based on predicted demand.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        route_id : str\n",
    "            ID of the route to optimize\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        recommendations : dict\n",
    "            Service frequency recommendations\n",
    "        \"\"\"\n",
    "        print(f\"Optimizing service frequency for route {route_id}...\")\n",
    "        \n",
    "        if self.demand_model is None:\n",
    "            print(\"Demand prediction model not trained.\")\n",
    "            return None\n",
    "        \n",
    "        # Get stops served by this route\n",
    "        if 'route_id' not in self.trips.columns:\n",
    "            print(\"Route information not available.\")\n",
    "            return None\n",
    "        \n",
    "        # Get trip IDs for this route\n",
    "        route_trips = self.trips[self.trips['route_id'] == route_id]['trip_id'].unique()\n",
    "        \n",
    "        if len(route_trips) == 0:\n",
    "            print(f\"No trips found for route {route_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Get stops served by these trips\n",
    "        route_stops = self.stop_times[self.stop_times['trip_id'].isin(route_trips)]['stop_id'].unique()\n",
    "        \n",
    "        # Predict demand for each hour\n",
    "        hourly_demand = []\n",
    "        \n",
    "        for hour in range(24):\n",
    "            # Get predictions for all stops on this route\n",
    "            hour_predictions = self.predict_system_demand(hour)\n",
    "            \n",
    "            # Filter for just this route's stops\n",
    "            route_demand = hour_predictions[hour_predictions['stop_id'].isin(route_stops)]\n",
    "            \n",
    "            # Calculate total route demand for this hour\n",
    "            total_demand = route_demand['predicted_demand'].sum()\n",
    "            hourly_demand.append({\n",
    "                'hour': hour,\n",
    "                'total_demand': total_demand,\n",
    "                'stops_count': len(route_demand)\n",
    "            })\n",
    "        \n",
    "        hourly_demand_df = pd.DataFrame(hourly_demand)\n",
    "        \n",
    "        # Calculate optimal frequency based on demand\n",
    "        # Simple formula: 1 trip per 20 passengers predicted\n",
    "        hourly_demand_df['recommended_frequency'] = (hourly_demand_df['total_demand'] / 20).round().astype(int)\n",
    "        \n",
    "        # Ensure minimum service levels (at least 1 trip per hour during service hours)\n",
    "        hourly_demand_df.loc[\n",
    "            (hourly_demand_df['hour'] >= 6) & \n",
    "            (hourly_demand_df['hour'] <= 22) & \n",
    "            (hourly_demand_df['recommended_frequency'] == 0),\n",
    "            'recommended_frequency'\n",
    "        ] = 1\n",
    "        \n",
    "        # Get route name\n",
    "        route_name = \"Unknown\"\n",
    "        if route_id in self.routes['route_id'].values:\n",
    "            route_name = self.routes[self.routes['route_id'] == route_id]['route_long_name'].iloc[0]\n",
    "        \n",
    "        # Generate recommendations\n",
    "        peak_hour = hourly_demand_df.loc[hourly_demand_df['total_demand'].idxmax()]\n",
    "        \n",
    "        recommendations = {\n",
    "            'route_id': route_id,\n",
    "            'route_name': route_name,\n",
    "            'stops_served': len(route_stops),\n",
    "            'peak_hour': int(peak_hour['hour']),\n",
    "            'peak_demand': peak_hour['total_demand'],\n",
    "            'peak_frequency': int(peak_hour['recommended_frequency']),\n",
    "            'hourly_recommendations': hourly_demand_df,\n",
    "            'avg_daily_trips': hourly_demand_df['recommended_frequency'].sum()\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def generate_ml_report(self):\n",
    "        \"\"\"Generate a comprehensive analysis report with ML insights.\"\"\"\n",
    "        print(\"Generating ML-enhanced transit analysis report...\")\n",
    "        \n",
    "        # Create a reports directory\n",
    "        reports_dir = os.path.join(self.data_dir, \"reports\")\n",
    "        os.makedirs(reports_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Route frequency analysis\n",
    "        if self.route_stats is not None:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='trip_count', y='route_long_name', data=self.route_stats.head(20))\n",
    "            plt.title('Top 20 Routes by Trip Frequency')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(reports_dir, 'route_frequency.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # 2. Generate demand predictions for key hours\n",
    "        key_hours = [8, 12, 17, 22]  # Morning rush, midday, evening rush, evening\n",
    "        \n",
    "        for hour in key_hours:\n",
    "            self.create_demand_heatmap(hour, f\"demand_heatmap_{hour}.html\")\n",
    "        \n",
    "        # 3. Identify underserved stops\n",
    "\n",
    "        underserved_stops = self.identify_underserved_stops()\n",
    "        if underserved_stops is not None:\n",
    "            underserved_stops.to_csv(os.path.join(reports_dir, 'underserved_stops.csv'), index=False)\n",
    "\n",
    "            # Check if we actually have any underserved stops before plotting\n",
    "            top_underserved = underserved_stops[underserved_stops['is_underserved']] if 'is_underserved' in underserved_stops.columns else pd.DataFrame()\n",
    "\n",
    "            if not top_underserved.empty and len(top_underserved) > 0:\n",
    "                # Plot top underserved stops\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                top_underserved = top_underserved.head(15)\n",
    "                sns.barplot(x='demand_service_ratio', y='stop_name', data=top_underserved)\n",
    "                plt.title('Top 15 Potentially Underserved Stops')\n",
    "                plt.xlabel('Demand to Service Ratio')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(reports_dir, 'underserved_stops.png'))\n",
    "            else:\n",
    "                # Create a figure with text explaining no underserved stops found\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.text(0.5, 0.5, \"No underserved stops identified\", \n",
    "                         horizontalalignment='center', verticalalignment='center',\n",
    "                         fontsize=16)\n",
    "                plt.axis('off')\n",
    "                plt.savefig(os.path.join(reports_dir, 'underserved_stops.png'))\n",
    "\n",
    "            plt.close()\n",
    "        \n",
    "        # 4. Stop clustering analysis\n",
    "        stop_clusters, cluster_stats = self.cluster_stops_by_location()\n",
    "        if stop_clusters is not None:\n",
    "            # Create a map with clusters\n",
    "            center_lat = stop_clusters['stop_lat'].mean()\n",
    "            center_lon = stop_clusters['stop_lon'].mean()\n",
    "            \n",
    "            cluster_map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "            \n",
    "            # Define colors for clusters\n",
    "            colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', \n",
    "                     'lightred', 'darkblue', 'darkgreen', 'cadetblue']\n",
    "            \n",
    "            # Add points colored by cluster\n",
    "            for _, row in stop_clusters.iterrows():\n",
    "                cluster_id = row['cluster']\n",
    "                color = colors[cluster_id % len(colors)]\n",
    "                \n",
    "                folium.CircleMarker(\n",
    "                    location=[row['stop_lat'], row['stop_lon']],\n",
    "                    radius=5,\n",
    "                    color=color,\n",
    "                    fill=True,\n",
    "                    fill_color=color,\n",
    "                    fill_opacity=0.7,\n",
    "                    popup=f\"Stop: {row['stop_name']}<br>Cluster: {cluster_id}<br>Activity: {row['activity_count']}\"\n",
    "                ).add_to(cluster_map)\n",
    "            \n",
    "            # Add cluster centers\n",
    "            for cluster_id, stats in cluster_stats.iterrows():\n",
    "                folium.Marker(\n",
    "                    location=[stats['stop_lat'], stats['stop_lon']],\n",
    "                    popup=f\"Cluster {cluster_id}<br>Stops: {stats['stop_count']}<br>Total Activity: {stats['activity_count']}\",\n",
    "                    icon=folium.Icon(color=colors[cluster_id % len(colors)], icon='info-sign')\n",
    "                ).add_to(cluster_map)\n",
    "            \n",
    "            cluster_map.save(os.path.join(reports_dir, 'stop_clusters.html'))\n",
    "        \n",
    "        # 5. Optimize top routes\n",
    "        if self.route_stats is not None:\n",
    "            top_routes = self.route_stats.head(5)['route_id'].tolist()\n",
    "            all_recommendations = []\n",
    "            \n",
    "            for route_id in top_routes:\n",
    "                recommendations = self.optimize_service_frequency(route_id)\n",
    "                if recommendations:\n",
    "                    all_recommendations.append(recommendations)\n",
    "            \n",
    "            if all_recommendations:\n",
    "                # Create plots for each route's hourly recommendations\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                for i, rec in enumerate(all_recommendations):\n",
    "                    plt.subplot(len(all_recommendations), 1, i+1)\n",
    "                    hourly = rec['hourly_recommendations']\n",
    "                    plt.bar(hourly['hour'], hourly['recommended_frequency'])\n",
    "                    plt.title(f\"Route: {rec['route_name']}\")\n",
    "                    plt.xlabel('Hour of Day')\n",
    "                    plt.ylabel('Recommended Trips')\n",
    "                    plt.xticks(range(0, 24, 2))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(reports_dir, 'route_optimization.png'))\n",
    "                plt.close()\n",
    "        \n",
    "        # 6. Create a summary report document\n",
    "        with open(os.path.join(reports_dir, 'ml_transit_analysis.txt'), 'w') as f:\n",
    "            f.write(\"ML-Enhanced Public Transit System Analysis\\n\")\n",
    "            f.write(\"=======================================\\n\\n\")\n",
    "            \n",
    "            f.write(\"1. System Overview\\n\")\n",
    "            f.write(f\"   - Total Routes: {len(self.routes) if self.routes is not None else 'N/A'}\\n\")\n",
    "            f.write(f\"   - Total Stops: {len(self.stops) if self.stops is not None else 'N/A'}\\n\")\n",
    "            f.write(f\"   - Total Trips: {len(self.trips) if self.trips is not None else 'N/A'}\\n\\n\")\n",
    "            \n",
    "            if self.route_stats is not None:\n",
    "                f.write(\"2. Route Analysis\\n\")\n",
    "                f.write(f\"   - Most Frequent Route: {self.route_stats.iloc[0]['route_long_name']} \"\n",
    "                       f\"({self.route_stats.iloc[0]['trip_count']} trips)\\n\")\n",
    "                f.write(f\"   - Least Frequent Route: {self.route_stats.iloc[-1]['route_long_name']} \"\n",
    "                       f\"({self.route_stats.iloc[-1]['trip_count']} trips)\\n\\n\")\n",
    "            \n",
    "            if self.demand_model is not None:\n",
    "                f.write(\"3. Demand Prediction Model\\n\")\n",
    "                f.write(f\"   - Model Type: {type(self.demand_model[-1]).__name__}\\n\")\n",
    "\n",
    "                # Instead of trying to re-evaluate the model (which would need X_test and y_test),\n",
    "                # just write out general information about the model\n",
    "                f.write(f\"   - Algorithm: Gradient Boosting Regression\\n\")\n",
    "                f.write(f\"   - Features: Location, time of day, and contextual variables\\n\")\n",
    "                f.write(f\"   - Important Time Periods: Morning Rush (7-9 AM), Evening Rush (4-6 PM)\\n\\n\")\n",
    "\n",
    "            if underserved_stops is not None and not underserved_stops.empty:\n",
    "                f.write(\"4. Service Gap Analysis\\n\")\n",
    "                underserved_count = underserved_stops['is_underserved'].sum()\n",
    "                f.write(f\"   - Potentially Underserved Stops: {underserved_count}\\n\")\n",
    "                if underserved_count > 0:\n",
    "                    top_underserved = underserved_stops[underserved_stops['is_underserved']].head(5)\n",
    "                    f.write(\"   - Top 5 Underserved Stops:\\n\")\n",
    "                    for _, row in top_underserved.iterrows():\n",
    "                        f.write(f\"     * {row['stop_name']} - Demand/Service Ratio: {row['demand_service_ratio']:.2f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if cluster_stats is not None:\n",
    "                f.write(\"5. Geographic Cluster Analysis\\n\")\n",
    "                f.write(f\"   - Number of Service Areas: {len(cluster_stats)}\\n\")\n",
    "                for cluster_id, stats in cluster_stats.iterrows():\n",
    "                    f.write(f\"   - Cluster {cluster_id}: {stats['stop_count']} stops, \"\n",
    "                           f\"{stats['activity_count']:.0f} total activity\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            if all_recommendations:\n",
    "                f.write(\"6. Route Optimization Recommendations\\n\")\n",
    "                for rec in all_recommendations:\n",
    "                    f.write(f\"   - Route {rec['route_name']}:\\n\")\n",
    "                    f.write(f\"     * Peak Hour: {rec['peak_hour']}:00\\n\")\n",
    "                    f.write(f\"     * Peak Demand: {rec['peak_demand']:.1f}\\n\")\n",
    "                    f.write(f\"     * Recommended Peak Frequency: {rec['peak_frequency']} trips/hour\\n\")\n",
    "                    f.write(f\"     * Recommended Daily Trips: {rec['avg_daily_trips']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"7. Recommendations\\n\")\n",
    "            f.write(\"   - Increase service frequency on high-demand routes during peak hours\\n\")\n",
    "            f.write(\"   - Address service gaps in underserved areas identified by the demand model\\n\")\n",
    "            f.write(\"   - Consider route modifications to better connect high-demand areas\\n\")\n",
    "            f.write(\"   - Implement real-time service adjustments based on predicted demand patterns\\n\")\n",
    "            f.write(\"   - Focus additional resources on morning and evening rush hours\\n\")\n",
    "        \n",
    "        print(f\"ML analysis report generated in {reports_dir}\")\n",
    "        return reports_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  MACHINE LEARNING ENHANCED PUBLIC TRANSIT ANALYSIS  \n",
      "======================================================================\n",
      "\n",
      "Step 1: Downloading transit data\n",
      "--------------------------------------------------\n",
      "Downloading GTFS data from https://api.mta.info/GTFS/nycbus/google_transit_bronx.zip...\n",
      "Error downloading data: File is not a zip file\n",
      "Using local data if available...\n",
      "\n",
      "Step 2: Loading transit data\n",
      "--------------------------------------------------\n",
      "Loading GTFS data...\n",
      "Loaded 14 routes and 186 stops.\n",
      "\n",
      "Step 3: Preprocessing data\n",
      "--------------------------------------------------\n",
      "Preprocessing data...\n",
      "Data preprocessing complete.\n",
      "\n",
      "Step 4: Analyzing transit patterns\n",
      "--------------------------------------------------\n",
      "Analyzed 12 routes\n",
      "\n",
      "Top 5 most frequent routes:\n",
      "  - Oakland Airport to Coliseum: 503 trips\n",
      "  - Coliseum to Oakland Airport: 502 trips\n",
      "  - Millbrae/SFIA to Antioch: 222 trips\n",
      "  - Antioch to SFIA/Millbrae: 220 trips\n",
      "  - Dublin/Pleasanton to Daly City: 168 trips\n",
      "\n",
      "Analyzed 50 stops\n",
      "\n",
      "Top 5 busiest stops:\n",
      "  - Coliseum: 1917 arrivals/departures\n",
      "  - Balboa Park: 1258 arrivals/departures\n",
      "  - Glen Park: 1258 arrivals/departures\n",
      "  - 24th Street / Mission: 1258 arrivals/departures\n",
      "  - 16th Street / Mission: 1258 arrivals/departures\n",
      "\n",
      "Step 5: Applying machine learning\n",
      "--------------------------------------------------\n",
      "\n",
      "Training demand prediction model...\n",
      "Training demand prediction model...\n",
      "Model Performance:\n",
      "  MAE: 2.49\n",
      "  RMSE: 3.52\n",
      "  R²: 0.9756\n",
      "Model R² score: 0.9756\n",
      "\n",
      "Clustering stops by geographic location...\n",
      "Clustering stops into 5 service areas...\n",
      "Service area clustering complete.\n",
      "\n",
      "Identified transit service areas:\n",
      "  - Area 0: 14.0 stops, 14686 total activity\n",
      "  - Area 1: 9.0 stops, 5166 total activity\n",
      "  - Area 2: 7.0 stops, 2826 total activity\n",
      "  - Area 3: 17.0 stops, 13858 total activity\n",
      "  - Area 4: 3.0 stops, 1737 total activity\n",
      "\n",
      "Identifying potentially underserved stops...\n",
      "Identifying potentially underserved stops...\n",
      "Predicting system-wide demand for hour 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Kagame\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Richard Kagame\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "<ipython-input-30-524ac78d7d48>:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stop_locations['cluster'] = kmeans.fit_predict(stop_locations[['stop_lat', 'stop_lon']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting system-wide demand for hour 17...\n",
      "Identified 0 potentially underserved stops\n",
      "\n",
      "Creating demand prediction heatmaps...\n",
      "Creating demand heatmap for hour 8...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Demand heatmap saved to data\\morning_rush_demand.html\n",
      "Creating demand heatmap for hour 12...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Demand heatmap saved to data\\midday_demand.html\n",
      "Creating demand heatmap for hour 17...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Demand heatmap saved to data\\evening_rush_demand.html\n",
      "\n",
      "Optimizing service frequency for top route...\n",
      "Optimizing service frequency for route 19...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2...\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "\n",
      "Service optimization for Route Oakland Airport to Coliseum:\n",
      "  - Peak hour: 12:00\n",
      "  - Peak demand: 165.3\n",
      "  - Recommended peak frequency: 8 trips/hour\n",
      "  - Recommended daily trips: 141\n",
      "\n",
      "Step 6: Generating analysis report\n",
      "--------------------------------------------------\n",
      "Generating ML-enhanced transit analysis report...\n",
      "Creating demand heatmap for hour 8...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Demand heatmap saved to data\\demand_heatmap_8.html\n",
      "Creating demand heatmap for hour 12...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Demand heatmap saved to data\\demand_heatmap_12.html\n",
      "Creating demand heatmap for hour 17...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Demand heatmap saved to data\\demand_heatmap_17.html\n",
      "Creating demand heatmap for hour 22...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Demand heatmap saved to data\\demand_heatmap_22.html\n",
      "Identifying potentially underserved stops...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Clustering stops into 5 service areas...\n",
      "Service area clustering complete.\n",
      "Optimizing service frequency for route 19...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard Kagame\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Richard Kagame\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "<ipython-input-30-524ac78d7d48>:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stop_locations['cluster'] = kmeans.fit_predict(stop_locations[['stop_lat', 'stop_lon']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "Optimizing service frequency for route 20...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2...\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "Optimizing service frequency for route 2...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2...\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "Optimizing service frequency for route 1...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2...\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "Optimizing service frequency for route 11...\n",
      "Predicting system-wide demand for hour 0...\n",
      "Predicting system-wide demand for hour 1...\n",
      "Predicting system-wide demand for hour 2...\n",
      "Predicting system-wide demand for hour 3...\n",
      "Predicting system-wide demand for hour 4...\n",
      "Predicting system-wide demand for hour 5...\n",
      "Predicting system-wide demand for hour 6...\n",
      "Predicting system-wide demand for hour 7...\n",
      "Predicting system-wide demand for hour 8...\n",
      "Predicting system-wide demand for hour 9...\n",
      "Predicting system-wide demand for hour 10...\n",
      "Predicting system-wide demand for hour 11...\n",
      "Predicting system-wide demand for hour 12...\n",
      "Predicting system-wide demand for hour 13...\n",
      "Predicting system-wide demand for hour 14...\n",
      "Predicting system-wide demand for hour 15...\n",
      "Predicting system-wide demand for hour 16...\n",
      "Predicting system-wide demand for hour 17...\n",
      "Predicting system-wide demand for hour 18...\n",
      "Predicting system-wide demand for hour 19...\n",
      "Predicting system-wide demand for hour 20...\n",
      "Predicting system-wide demand for hour 21...\n",
      "Predicting system-wide demand for hour 22...\n",
      "Predicting system-wide demand for hour 23...\n",
      "ML analysis report generated in data\\reports\n",
      "\n",
      "Analysis complete!\n",
      "Results are available in the data\\reports directory\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Machine Learning Enhanced Transit Analysis\n",
    "# Example usage script\n",
    "\n",
    "#from ml_transit_analyzer import MLTransitAnalyzer\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  MACHINE LEARNING ENHANCED PUBLIC TRANSIT ANALYSIS  \")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize the analyzer\n",
    "    analyzer = MLTransitAnalyzer()\n",
    "    \n",
    "    # Step 1: Download GTFS data (or use a local copy)\n",
    "    print(\"\\nStep 1: Downloading transit data\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # New York MTA GTFS data for subway service\n",
    "        analyzer.download_gtfs(\"https://api.mta.info/GTFS/nycbus/google_transit_bronx.zip\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        print(\"Using local data if available...\")\n",
    "    \n",
    "    # Step 2: Load the data\n",
    "    print(\"\\nStep 2: Loading transit data\")\n",
    "    print(\"-\" * 50)\n",
    "    data_loaded = analyzer.load_data()\n",
    "    \n",
    "    if not data_loaded:\n",
    "        print(\"Error: Could not load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Preprocess data for machine learning\n",
    "    print(\"\\nStep 3: Preprocessing data\")\n",
    "    print(\"-\" * 50)\n",
    "    analyzer.preprocess_data()\n",
    "    \n",
    "    # Step 4: Analyze basic transit patterns\n",
    "    print(\"\\nStep 4: Analyzing transit patterns\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get route frequency\n",
    "    route_freq = analyzer.analyze_route_frequency()\n",
    "    print(f\"Analyzed {len(route_freq)} routes\")\n",
    "    print(\"\\nTop 5 most frequent routes:\")\n",
    "    for _, row in route_freq.head(5).iterrows():\n",
    "        print(f\"  - {row['route_long_name']}: {row['trip_count']} trips\")\n",
    "    \n",
    "    # Get stop activity\n",
    "    stop_activity = analyzer.analyze_stop_activity()\n",
    "    print(f\"\\nAnalyzed {len(stop_activity)} stops\")\n",
    "    print(\"\\nTop 5 busiest stops:\")\n",
    "    for _, row in stop_activity.head(5).iterrows():\n",
    "        print(f\"  - {row['stop_name']}: {row['activity_count']} arrivals/departures\")\n",
    "    \n",
    "    # Step 5: Apply machine learning\n",
    "    print(\"\\nStep 5: Applying machine learning\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train demand prediction model\n",
    "    print(\"\\nTraining demand prediction model...\")\n",
    "    model_perf = analyzer.train_demand_prediction_model()\n",
    "    \n",
    "    if model_perf:\n",
    "        print(f\"Model R² score: {model_perf['r2']:.4f}\")\n",
    "    \n",
    "    # Cluster stops by location\n",
    "    print(\"\\nClustering stops by geographic location...\")\n",
    "    stop_clusters, cluster_stats = analyzer.cluster_stops_by_location(n_clusters=5)\n",
    "    \n",
    "    if cluster_stats is not None:\n",
    "        print(\"\\nIdentified transit service areas:\")\n",
    "        for cluster_id, stats in cluster_stats.iterrows():\n",
    "            print(f\"  - Area {cluster_id}: {stats['stop_count']} stops, {stats['activity_count']:.0f} total activity\")\n",
    "    \n",
    "    # Identify underserved stops\n",
    "    print(\"\\nIdentifying potentially underserved stops...\")\n",
    "    underserved = analyzer.identify_underserved_stops()\n",
    "    \n",
    "    if underserved is not None:\n",
    "        underserved_count = underserved['is_underserved'].sum()\n",
    "        print(f\"Identified {underserved_count} potentially underserved stops\")\n",
    "        \n",
    "        if underserved_count > 0:\n",
    "            print(\"\\nTop 5 most underserved stops:\")\n",
    "            top_underserved = underserved[underserved['is_underserved']].head(5)\n",
    "            for _, row in top_underserved.iterrows():\n",
    "                print(f\"  - {row['stop_name']}: Demand/Service Ratio = {row['demand_service_ratio']:.2f}\")\n",
    "    \n",
    "    # Create demand heatmap for key hours\n",
    "    print(\"\\nCreating demand prediction heatmaps...\")\n",
    "    morning_rush = analyzer.create_demand_heatmap(8, \"morning_rush_demand.html\")\n",
    "    midday = analyzer.create_demand_heatmap(12, \"midday_demand.html\")\n",
    "    evening_rush = analyzer.create_demand_heatmap(17, \"evening_rush_demand.html\")\n",
    "    \n",
    "    # Optimize route frequency\n",
    "    if route_freq is not None and len(route_freq) > 0:\n",
    "        print(\"\\nOptimizing service frequency for top route...\")\n",
    "        top_route_id = route_freq.iloc[0]['route_id']\n",
    "        top_route_name = route_freq.iloc[0]['route_long_name']\n",
    "        \n",
    "        recommendations = analyzer.optimize_service_frequency(top_route_id)\n",
    "        \n",
    "        if recommendations:\n",
    "            print(f\"\\nService optimization for Route {top_route_name}:\")\n",
    "            print(f\"  - Peak hour: {recommendations['peak_hour']}:00\")\n",
    "            print(f\"  - Peak demand: {recommendations['peak_demand']:.1f}\")\n",
    "            print(f\"  - Recommended peak frequency: {recommendations['peak_frequency']} trips/hour\")\n",
    "            print(f\"  - Recommended daily trips: {recommendations['avg_daily_trips']}\")\n",
    "    \n",
    "    # Step 6: Generate comprehensive report\n",
    "    print(\"\\nStep 6: Generating analysis report\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    report_dir = analyzer.generate_ml_report()\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Results are available in the {report_dir} directory\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
